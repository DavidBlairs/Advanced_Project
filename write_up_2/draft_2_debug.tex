\documentclass[a4paper,12pt]{book}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{helvet}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[style = numeric, backend = bibtex]{biblatex}
\graphicspath{ {./images/} }

\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\operation}{\bigotimes}
\newcommand{\matrx}[1]{\bm{#1}}
\newcommand{\vectr}[1]{\textbf{#1}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\natral}{\mathbb{N}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\italic}[1]{\textit{#1}}
\newcommand{\rrank}[1]{rk_{row}(\matrx{#1})}
\newcommand{\crank}[1]{rk_{col}(\matrx{#1})}
\newcommand{\nexteq}{\refstepcounter{equation}\theequation}
\newcommand{\dett}[1]{\text{det}(\matrx{#1})}
\newcommand{\mdett}[1]{\text{det} \left ( #1 \right )}
\newcommand{\lcomb}[1]{\sum_{i = 0}^{k} {#1} \vectr{v}_i}
\newcommand{\vecspace}[1]{#1}

% new environments 
% recto  inverso 
% set book class to one sided
% add chapter levels
% new environment / draft notes / if then else draft = 1 / check the internet for prior examples

\doublespacing
\setstretch{1.5}

\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]

\addbibresource{sources/sources.bib}

\begin{document}
	\section{Vector Space}
	% vector space introduction
	A vector space is an algebraic structure that consists of a set of vectors and two operations defined on these vectors: vector addition and scalar multiplication. Vectors can be added together and multiplied by numbers, called scalars. The operations of vector addition and scalar multiplication must satisfy a set of axioms, which ensure that the resulting vectors remain within the vector space. It will be helpful to describe a vector space by first explaining the concept of a set and a group. 
	
	% set definition
	A set is a collection of mathematical objects such as numbers, lines or potentially other sets. They were first formalised by George Cantor in 
	1895 \cite{cantor1895beitrage} as being either infinite or finite and containing distinct elements. 
	% group definition
	\begin{definition}
		\normalfont
		Let $ \mathcal{V} $ be a set and $ \operation $ denote a binary operation between elements of this set such that $ \operation : \set{G} \times \set{G} \rightarrow \set{G} $. $G := (\set{G}, \operation)$ is called a group  \cite[page 36]{mml_book} if the following conditions are met:
		\begin{enumerate}
			\item (Closure) The binary operation between any two elements of the set will result in an element which is also part of the set: $ \operation : \forall x,y \in \set{G} : x \operation y \in \set{G} $.
			\item (Associativity) The order in which group operations are performed on a set of elements is irrelevant: $ \forall x, y, z \in \set{G} : (x \operation y) \operation z = x \operation (y \operation z) $.
			\item (Neutral Element) A neutral element in a set is an element that, when combined with any other element in the set using the group operation, results in the same element: $ \exists e  \in \set{G} \forall x \in \set{G} : x \operation e = x$ and $ e \operation x = x $.
			\item (Inverse Element) The inverse of an element in a set is an element that when combined with the original element using the group operation, results in the neutral element. It allows for the reversal of the group operation: $ \forall x \in \set{G} \exists y \in \set{G} :  x \operation y = e$ and $ y \operation x = e $, where $ e $ is the neutral element. We can denote the inverse of an element $ x $ as $ x^{-1} $.
		\end{enumerate}
	\end{definition}
		
	% relevence of groups 
	Groups are very important in many areas of mathematics. Their rigorous and formal definition means that, if something is found to be a group, its properties can be better understood in the specific context in which it is relevant.
	% example of group 
	% ammend: general linear group being in italic / reference from the book / regular (invertible) not regular invertible
	An example of a group is the general linear group. This is the set of regular invertible matrices $ \matrx{A} \in \real^{n \times n} $ where the group operation is matrix multiplication. The inverse elements would be the inverse of the matrix and the neutral element would be the identity matrix. However, the order in which the group operation is performed will affect the result i.e $ \matrx{AB} \ne \matrx{BA} $. 
	% definition of an abelian group
	Given a group $ G $, if the order in which the group operation is performed does not matter i.e. $ \forall x, y \in \set{G} : x \operation y = y \operation x $, then $ G = (\set{G}, \operation) $ is known as an \italic{Abelian group} (commutative). An example of this would be $ (\integers, +) $, the set of all integers under the addition operation.
	
	
	% definition of a vector space
	A vector space \cite[page 37]{mml_book} is a special type of group with some additional conditions. 
	
	\begin{definition}[Vector Space]
		\normalfont A real-valued $\italic{vector space}$ $V = (\set{V}, +, \cdot)$ is a set $\set{V}$ with two operations:
		\begin{align}
			+: \set{V} &\times \set{V} \rightarrow \set{V} \hspace{10pt} (\text{Inner Operation}) \\
			\cdot: \real &\times \set{V} \rightarrow \set{V} \hspace{10pt} (\text{Outer Operation})
		\end{align}
		where:
		\begin{enumerate}
			\item $(\set{V}, +)$ is an Abelian group
			\item (Distributivity) The outer operation can be ``Distributed'' across elements either before or after the inner operation has occurred: 
			\begin{enumerate}
				\item $\forall \lambda \in \real, \vectr{x}, \vectr{y} \in \mathcal{V}: \lambda (\vectr{x} + \vectr{y}) = (\lambda \cdot \vectr{x} + \lambda \cdot \vectr{y})$
				\item $\forall \lambda, \psi \in \real, \vectr{x} \in \set{V}: (\lambda + \psi) \cdot \vectr{x} = \lambda \cdot \vectr{x} + \psi \cdot \vectr{x}$
			\end{enumerate} 
			\item Associativity (outer operation): $\forall \lambda, \psi \in \real, \vectr{x} \in \set{V}: \lambda \cdot (\psi \cdot \vectr{x}) = (\lambda \psi) \cdot \vectr{x}$
			\item Neutral element with respect to the outer operation: $\forall \vectr{x} \in \set{V}: 1 \cdot \vectr{x} = \vectr{x}$
		\end{enumerate}
		\label{def:vector_space}
	\end{definition}
	It is also the case that non-real valued vector spaces are permitted. 
	
	% generalisation of a vector
	We commonly think of vectors as being mathematical objects with both direction and magnitude. However, these are only geometric vectors. More generally, any set of objects which follows the definition of a vector space is known as a vector. For example, polynomials of the same degree are also vectors. Two can be added together, resulting in another polynomial of the same degree and they can be multiplied by a scalar $ \lambda \in \real $ which again results in another polynomial. 
	
	% ammend: add some more details here about how the specific examples fit into the axioms of a vector space. You should also include an example of a finite dimensional and infinite dimensional situation.
	
	
	% begin linear independence
	\section{Linear Independence}
	Linear independence \cite[page 40]{mml_book} is a property of a set of vectors which describes whether there is any redundancy with respect to the linear combinations of these vectors. 
	% definition of linear combination
	\begin{definition}[Linear Combination]
		\normalfont	Consider a vector space $\italic{V}$ and a finite number of vectors $\vectr{x}_i,\ldots,\vectr{x}_k \in \italic{V}$. Then, every $\vectr{v} \in \italic{V}$ of the form
		\begin{align}
			\vectr{v} = \lambda_1\vectr{x}_1 + \cdots + \lambda_k\vectr{x}_k =  \sum_{i=1}^{k} \lambda_i\vectr{x}_i \in \italic{V}
		\end{align}
		with $\lambda_1,\ldots,\lambda_k \in \real$ is a $\italic{linear combination}$ of the vectors $\vectr{x}_1,\cdots,\vectr{x}_k$.
		\label{def:linear_combinations}
	\end{definition}
	% base vectors of R^2
	Consider two vectors in $ \real^2 $, $\vectr{e}_1 = (1, 0)^{\top}$ and $\vectr{e}_2 = (0, 1)^{\top}$. It is common to see these written as $ \vectr{i} $ and $ \vectr{j} $ respectively. We can represent any vector in $ \real^2 $ as a linear combination of these two vectors: \\
	\italic{Proof}:
	We can express any vector in $ \real^{2} $ as $ \vectr{v} = (\alpha, \beta)^{\top} $ where $ \alpha, \beta \in \real $. It is the case that $ \matrx{I}\vectr{v} $ = $ \vectr{v} $ for all $ \vectr{v} $. Therefore:
	\begin{align}
		\matrx{I}\vectr{v} = \vectr{v} = \begin{bmatrix}
			1 & 0 \\
			0 & 1 
		\end{bmatrix} \begin{bmatrix}
			\alpha \\
			\beta 
		\end{bmatrix} = \alpha \begin{bmatrix}
			1 \\
			0 
		\end{bmatrix} + \beta \begin{bmatrix}
			0 \\
			1
		\end{bmatrix} = \alpha \vectr{e}_1 + \beta \vectr{e}_2
	\end{align}
	% definition of linear independence
	\begin{definition}[Linear Dependence] 
		\normalfont Consider a vector space $\italic{V}$ with $k \in \natral$ and  $\vectr{x}_1,\cdots,\vectr{x}_k \in \italic{V}$. If there is a non-trivial combination, such that $\vectr{0} = \sum_{i=1}^{k} \lambda_i\vectr{x}_i$ with at least one $\lambda_i \ne 0$, the vectors  $\vectr{x}_1,\cdots,\vectr{x}_k$ are \italic{linearly dependent}. \label{def:linear_independence}
	\end{definition} 
	% definition of linear dependence
	\begin{definition}[Linear Independence]
		\normalfont Consider a vector space $\italic{V}$ with $k \in \natral$ and  $\vectr{x}_1,\cdots,\vectr{x}_k \in \italic{V}$. If only the trivial solution exists, such that $\vectr{0} = \sum_{i=1}^{k} \lambda_i\vectr{x}_i$ with all $\lambda_i = 0$, the vectors $\vectr{x}_1,\cdots,\vectr{x}_k $ are \italic{linearly independent}.
		\label{def:linear_independence}
	\end{definition}
		% example of linear independence through linear combinations
	Lets consider the following set $\set{V}$ of vectors $\vectr{v}_1,\vectr{v}_2 $ and $ \vectr{v}_3$ where (respectively):
	\begin{align}
		\set{V} = \left\{\begin{bmatrix} 1 \\ 2 \end{bmatrix},\begin{bmatrix} -2 \\ -4 \end{bmatrix},\begin{bmatrix} -4 \\ -1 \end{bmatrix} \right\}
	\end{align}
	And let $\vectr{y} \in \real^2$ be any linear combination that can be made from these vectors. Writing this out explicitly with $\alpha, \beta, \omega \in \real$:
	\begin{align}
		\vectr{y} &= \alpha \begin{bmatrix} 1 \\ 2 \end{bmatrix} + \beta \begin{bmatrix} -2 \\ -4 \end{bmatrix} + \omega \begin{bmatrix} -4 \\ -1 \end{bmatrix} \\
		&= \alpha \begin{bmatrix} 1 \\ 2 \end{bmatrix} - 2\beta \begin{bmatrix} 1 \\ 2 \end{bmatrix} + \omega \begin{bmatrix} -4 \\ -1 \end{bmatrix} \\
		&= (\alpha - 2\beta) \begin{bmatrix} 1 \\ 2 \end{bmatrix} + \omega \begin{bmatrix} -4 \\ -1 \end{bmatrix} \\
	\end{align} 
	% intuition behind linear independence 
	What we have shown is that if a vector $ \vectr{y} $ can be represented as a linear combination of $ \vectr{v}_1,\vectr{v}_2 $ and $ \vectr{v}_3 $ then we can represent $ \vectr{y} $ as a linear combination of $ \vectr{v}_2 $ and $ \vectr{v}_3 $. This is because $ \vectr{v}_1 $ is a scaled version of $ \vectr{v}_2 $ and vice versa. 
	% leading into the formal definition 
	More formally:
	
	\begin{align}
		\begin{bmatrix} -2 \\ -4 \end{bmatrix} + 2 \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \vectr{0}
	\end{align}
	
	which following from definition \ref{def:linear_independence} demonstrates that we have a linearly dependent set of vectors.
	\section{Generating Set and Span}
	
	A generating set and the span \cite[page 44]{mml_book} of a set of vectors are concerned with the vector space produced by the linear combination of all the vectors in the set.
	
	% definition of a generating set and span
	\begin{definition}[Generating Set and Span]
		\normalfont Consider a vector space $\italic{V} = (\set{V}, +, \cdot)$ and set of vectors $\set{A} = \{\vectr{x}_1,\ldots,\vectr{x}_k\} \subseteq \italic{V}$. If every vector $\vectr{v} \in \italic{V}$ can be expressed as a linear combination of vectors of $\vectr{x}_1,\ldots,\vectr{x}_k$, $\set{A}$ is called a generating set of $\italic{V}$. The set of all linear combinations of vectors in $\set{A}$ is called the span of $\set{A}$. If $\set{A}$ spans the vector space $\italic{V}$, we write $\italic{V} = span[\set{A}]$. 
		\label{def:generating_set_span}
	\end{definition}
	
	% generating set of R^2
	Consider the vector space $ V = (\real^{2}, +, \cdot) $ under vector addition and scalar multiplication. Previously we have said that every vector $ v \in V $ can be represented as a linear combination of $\vectr{e}_1 = (1, 0)^{\top}$ and $\vectr{e}_2 = (0, 1)^{\top}$. By definition \ref{def:generating_set_span}, the set of vectors $ \set{B} = \{ \vectr{e}_1, \vectr{e}_2 \} $ is a generating set of $ V $. 
	% span of base vectors of R^2
	On the other hand, the set of two-dimensional vectors $ \real^{2} $ contains all linear combinations of the set $ \set{B} $ meaning that the $ span[\set{B}] = \real^{2} $. In this sense, a generating set and the span of a set are two sides of the same coin.
	\section{Basis}
	
	% reasoning behind a basis
	A basis is concerned with the minimum number of vectors that would be needed to span a particular vector space. 
	% basis definition
	\begin{definition}[Basis]
		\normalfont Consider a vector space $\italic{V} = (\set{V}, +, \cdot)$ and $\set{A} \subseteq \set{V}$. A generating set $\set{A}$ of $\italic{V}$ is called $\italic{minimal}$ if there exists no smaller set (with a lower cardinality) $\set{\tilde{A}} \subseteq \set{A} \subseteq \set{V}$ that spans $\italic{V}$. Every linearly independent generating set of $\italic{V}$  is minimal and is called a $\italic{basis}$ of $\italic{V}$.
		\label{def:basis}
	\end{definition}
	% basis example
	Consider the vector space $ \italic{V} = (\real^{3}, +, \cdot) $. A potential generating set for $ \italic{V} $ is the set $ \set{A} $ where:
	% basis vectors for R^3
	\begin{align}
		\set{A} = \left\{\begin{bmatrix} 1 \\ 0 \\ 0  \end{bmatrix},\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix},\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} \right\}
	\end{align}
	% informal explanation for a basis
	This is because every vector in $ \real^{3} $ can be represented as linear combinations of the vectors in $ \set{A} $:
	\italic{Proof}: Given a vector $ \vectr{v} \in \real^{3} $:
	\begin{align}
		\vectr{v} = \matrx{I}\vectr{v} = \begin{bmatrix}
			1 & 0 & 0 \\
			0 & 1 & 0 \\
			0 & 0 & 1 
		\end{bmatrix} \begin{bmatrix}
			v_1 \\
			v_2 \\
			v_3
		\end{bmatrix} = v_1 \begin{bmatrix}
			1 \\ 0 \\ 0
		\end{bmatrix} + v_2 \begin{bmatrix}
			0 \\ 1 \\ 0 
		\end{bmatrix} + v_3 \begin{bmatrix}
			0 \\ 0 \\ 1
		\end{bmatrix}
	\end{align}
	
	Since this set of vectors is linearly independent, it forms a \italic{minimal} generating set. We've described previously that the $ span $ of a set of vectors can be thought of as the collection of all potential linear combinations of the set. $ \set{A} $ forms a \italic{minimal} generating set because no vectors may be removed without eliminating some linear combinations thus reducing the overall span of the set. This is what we mean by a basis for the vector space $ V $. 
	% dimensionality = number of basis
	As well as this, the number of dimensions of the vector space spanned by the basis is defined as the number of vectors in the basis for the vector space.
	\section{Row Rank and Column Rank}
	
	% proof that the column rank = dim(span(columns vectors of A))
	Given a matrix $ \matrx{A} \in \real^{n \times m} $, the row rank (denoted as $ \rrank{A} $) and the column rank (denoted as $ \crank{A} $) describe the number of linearly independent row vectors and column vectors of the matrix $ \matrx{A} $ respectively.
	% proposition for dim = rank
	\begin{definition}
		\normalfont For a matrix $ \matrx{A} \in \real^{n \times m} $ where $ \matrx{A}_{.i} $ denotes the $ i $th column vector:
		\begin{align}
			\crank{A} = dim(span[\{\matrx{A}_{.1}, \ldots, \matrx{A}_{.m}\}])
		\end{align}
		\label{prop:c_rank_dim}
	\end{definition}
	% example for 2x2 case
	\noindent\italic{Example:}
	Consider a matrix $\matrx{A} = [\matrx{A}_{.1} | \matrx{A}_{.2}]$ where $ \matrx{A}_{.1} $ and $ \matrx{A}_{.2} $ are linearly dependent. The $ span $ of $\set{B} = \{\matrx{A}_{.1}, \matrx{A}_{.2}\} $ is all the linear combinations that can be created from the two vectors. Let $ s \in span[B] $:  
	\begin{align}
		s = \lambda_1 \matrx{A}_{.1} + \lambda_2 \matrx{A}_{.2}, \hspace{10pt}\forall \lambda_1, \lambda_2 \in \real
	\end{align}
	Because the column vectors are linearly dependent, by definition \ref{def:linear_independence}, $ \exists \lambda_3, \lambda_4$ where $ \lambda_3,\lambda_4 \ne 0 $ such that: 
	\begin{align}
		\lambda_3 \matrx{A}_{.1} + \lambda_4 \matrx{A}_{.2} &= \vectr{0} \\
		\matrx{A}_{.1} &= \frac{\lambda_4}{\lambda_3} \matrx{A}_{.2}
	\end{align}
	Meaning that every linear combination can be written as:
	\begin{align}
		s &= \lambda_1 \matrx{A}_{.1} + \lambda_2 \matrx{A}_{.2} \\
		&= (-\lambda_1 \frac{\lambda_4}{\lambda_3} + \lambda_4) \matrx{A}_{.2}
	\end{align}
	This means that $ \set{C} = \{\matrx{A}_{.2}\} $ forms a basis for the set of column vectors of $ \matrx{A} $. Because the number of elements in this set is 1, the dimension of $ span[C] $ is 1. Because both column vectors of $ \matrx{A} $ are linearly independent, the $ \crank{A} = 1$.
	% row rank = column rank proposition
	\begin{proposition}
		\normalfont Given a matrix $ \matrx{A} \in \real^{n \times m}$, the following statement is always true:
		\begin{align}
			\rrank{A} = \crank{A}
			\label{eq:row_rank_col_rank}
		\end{align}
		where $ \rrank{A} $ and $ \crank{A} $ denote the row rank and the column rank respectively.
		\label{prop:row_rank_col_rank}
	\end{proposition}

	% proof that the row rank is equal to the column rank
	\noindent\italic{Proof:}
	Let $ \matrx{A} \in \real^{m \times n}$ be a matrix and let $ u = \crank{A} $. There exists a basis $ C = \{\vectr{c}_1, \ldots, \vectr{c}_u\} $ of $ m \times 1 $ column vectors that spans the same space spanned by the columns of $ \vectr{A} $. 
	Let $ \matrx{B} = [\vectr{c}_1 | \vectr{c}_2 | \ldots | \vectr{c}_{u - 1} \vectr{c}_u] $ be a matrix where $ [\vectr{v}_1 | \vectr{v}_2] $ denotes the concatenation between two vectors $ \vectr{v}_1 $ and $ \vectr{v}_2 $. By definition \ref{def:linear_independence}, each column of $ \matrx{A} $ can be expressed as a linear combination of vectors in $ \matrx{B} $. If we collect the coefficients of these linear combinations into a matrix $ \matrx{D} $, we can represent $ \matrx{A} $ as $ \matrx{A} = \matrx{BD} $ where $ \matrx{D} \in \real^{m \times n}$. Equally, we can see that the rows of $ \matrx{BD} $ can be expressed as a linear combination of $ \matrx{D} $ with coefficients taken from $ B $. $ \therefore $, the span of the rows of $ \matrx{A} $ is no greater than the span of the rows of $ \matrx{D} $ because linear combinations of the rows of $ \matrx{A} $ can be written as linear combinations of the rows in $ \matrx{D} $. $ \matrx{D} $ has $ u $ rows. If they are linearly independent, the dimension of their span is $ u $. Otherwise, it has dimension $ < u $. $ \therefore $ the row rank is $ \leq u $ and so $ \rrank{A} \leq \crank{A} $ for any matrix $ \matrx{A} \in \real^{m \times n} $. If we let $ \matrx{K} \in \real^{n \times m} $ such that $ \matrx{K} = \matrx{A}^{\top} $, our previous result demonstrates that $ \rrank{K} \leq \crank{K} $. Because $ \matrx{K} = \matrx{A}^{\top} $, this means that $ \crank{A} \leq \rrank{A} $. This means that $ \crank{A} \leq \rrank{A} \leq \crank{A} $ and so proposition \ref{prop:row_rank_col_rank} that the $ \crank{A} = \rrank{A} $ is proved. 
	
	Bringing it all together, we get the definition for the rank of a matrix:
	% definition of rank
	\begin{definition}[Rank]
		\normalfont The number of linearly independent columns of a matrix $\textbf{A} \in \mathbb{R}^{m \times n}$ equals the number of linearly independent rows and is called the $\textit{rank}$ of $\textbf{A}$ and is denoted by $rk(\textbf{A})$.
	\end{definition}

	\section{Row Space and Column Space}
	
	Given a set of vectors $ \set{C} $, the $span[C]$ is the set of all possible linear combinations of vectors in $ \set{C} $. This also forms a vector space with the set $ \set{C} $ which in some sense is a consequence of the definition of the $ span $ being linear combinations of vectors. 
	
	\begin{proposition}
		\normalfont Given a set of vectors $ \set{S} = \{v_1, v_2, \ldots, v_{k - 1}, v_k\} $, the $ span[\set{S}] $ forms a vector space.
		\label{prop:span_is_vector_space}
	\end{proposition}
	\italic{Proof}: From definition \ref{def:generating_set_span} we know that all elements in $ span[\set{S}] $ can be represented as linear combinations of vectors in $ \set{S} $
	\begin{align}
		\lcomb{a_i} \in \set{S}, \forall a_1, a_2, \ldots a_k \in \real
	\end{align}
	For $ span[\set{S}] $ to be a vector space, it must meet certain conditions:
	\begin{enumerate}
		\item $ (\set{S}, +) $ forms an Abelian Group (commutative). This means it must meet the conditions for a group and be commutative:
		\begin{enumerate}
			\item (Closure)  $ \operation : \forall x,y \in \set{G} : x \operation y \in \set{G} $:
			\begin{align}
				\lcomb{a_i} + \lcomb{b_i} = \left[ \lcomb{(a_i + b_i)} \right] \in \set{S}, \forall a_i, b_i \in \real 
			\end{align}
			\item (Associativity) $ \forall x, y, z \in \set{G} : (x \operation y) \operation z = x \operation (y \operation z) $:
			\begin{align}
				\lcomb{a_i} + (\lcomb{b_i} + \lcomb{c_i}) = \lcomb{(a_i + b_i + c_i)} &=\\ (\lcomb{a_i} + \lcomb{b_i}) + \lcomb{c_i}, \forall a_i,b_i,c_i \in \real
			\end{align}
			\item (Neutral Element) $ \exists e  \in \set{G} \forall x \in \set{G} : x \operation e = x$ and $ e \operation x = x $:
			\begin{align}
				\lcomb{a_i} + \lcomb{(0)} = \lcomb{(0)} + \lcomb{a_i} = \lcomb{a_i}
			\end{align}
			\item (Inverse)  $ \forall x \in \set{G} \exists y \in \set{G} :  x \operation y = e$ and $ y \operation x = e $ where $ e $ is the inverse element: 
			\begin{align}
				\lcomb{a_i} + \lcomb{-a_i} = \lcomb{-a_i} + \lcomb{a_i} =\\ \lcomb{(a_i - a_i)} = \lcomb{(0)}
			\end{align}
			\item (Commutativity) $ \forall x, y \in \set{G}, x \operation y = y \operation x $:
			\begin{align}
				\lcomb{a_i} + \lcomb{b_i} = \lcomb{b_i} + \lcomb{a_i} = \lcomb{(a_i + b_i)}
			\end{align}
		\end{enumerate}
		Therefore, $ (\set{S}, +) $ forms an Abelian Group. 
		\item (Distributivity):
		\begin{enumerate}
			\item $\forall \lambda \in \real, \vectr{x}, \vectr{y} \in \mathcal{V}: \lambda (\vectr{x} + \vectr{y}) = (\lambda \cdot \vectr{x} + \lambda \cdot \vectr{y})$:
			\begin{align}
				\lambda \left[ \lcomb{a_i} + \lcomb{b_i} \right] = \lambda \lcomb{a_i} + \lambda \lcomb{b_i}, \forall \lambda \in \real
			\end{align}
			\item $\forall \lambda, \psi \in \real, \vectr{x} \in \set{V}: (\lambda + \psi) \cdot \vectr{x} = \lambda \cdot \vectr{x} + \psi \cdot \vectr{x}$:
			\begin{align}
				(\lambda + \phi)\lcomb{a_i} = \lambda \lcomb{a_i} + \phi \lcomb{a_i}
			\end{align}
		\end{enumerate} 
		\item (Neutral Element) $ \exists e  \in \set{G} \forall x \in \set{G} : x \operation e = x$ and $ e \operation x = x $: 
		\begin{align}
			1 \cdot \lcomb{a_i} = \lcomb{a_i}
	 	\end{align}
	\end{enumerate}
	Thus demonstrating that the $ span[\set{S}] $ is a vector space and proving proposition \ref{prop:span_is_vector_space}. 
	
	We need to introduce two more definitions: 
	% definition of row space ammend: 
	\begin{definition}[Row Space]
		\normalfont Let $\set{C}$ be a set of vectors of length $n$ where each vector is a row of a matrix $\matrx{A} \in \real^{m \times n}$. The row space of the matrix $\set{A}$ is the vector space of the $span$ of the set of vectors $\set{C}$. 
	\end{definition}
	% definition of column space ammend:
	\begin{definition}[Column Space]
		\normalfont Let $\set{C}$ be a set of vectors of length $m$ where each vector is a column of a matrix $\matrx{A} \in \real^{m \times n}$. The column space of the matrix $\matrx{A}$ is the vector space of the $span$ of the set of vectors $\set{C}$. 
	\end{definition}
	% row space is not equal to the column space
	\begin{proposition}[Row Space and Column Space]
		\normalfont Given a matrix $ \matrx{A} \in \real^{m \times n} $, the row space is not always equal to the column space.
	\end{proposition}
	% proof of previous claim
	\italic{Proof}: Let $ \matrx{A} $ be a matrix defined as follows with the set of all row vectors being $ \set{R} $ and the set of all column vectors being $ \set{C} $:
	
	\begin{align}
		\matrx{A} = \begin{bmatrix}
			1 & 3 \\
			2 & 2 \\
			3 & 1 
		\end{bmatrix}
	\end{align} 

	The column space contains a vector $ \vectr{v}_1 $ such that:
	
	\begin{align}
		\vectr{v}_1 = \begin{bmatrix}
			1 \\
			2 \\
			3 
		\end{bmatrix} + \begin{bmatrix}
		3 \\
		2 \\
		1 
	\end{bmatrix} = \begin{bmatrix}
		4 \\
		4 \\
		4 
	\end{bmatrix}
 	\end{align}
	It is clear that there exists no linear combination of the row vectors that could result in the vector $ \vectr{v}_1 $. Therefore, the $ span[\set{R}] $ does not contain a vector in $ span[\set{C}] $ and they cannot be equal. More generally, the row space and column space may not always be the same since they do not contain the same set of vectors. 
	% brief intro to eignevectors and eigenvalues
	\section{Eigenvalues and Eigenvectors} \label{sec:eigen}
	One of the most important properties of a matrix in potentially all of linear algebra is the calculation / study of eigenvalues and eigenvectors. Its hard to overstate their significance. We will introduce them here and discuss a method for calculating them at a later point. 
	\\\\
	% definition of an eigenvalue and eigenvector
	\begin{definition}
		\normalfont Given a matrix $ \matrx{A} \in \real^{n \times n} $ where $ n \in \natral $ and two variables, a vector $ \vectr{u} \in \real^{n} $ and a scalar $ \lambda \in \real $, if these variables solve the following equation, $ \lambda $ is an ``eigenvalue" and $ \vectr{u} $ is an ``eigenvector" of $ \lambda $.
		\begin{align}
			\matrx{A} \vectr{u} = \lambda \vectr{u}
		\end{align}
		There may be up to $ n $ eigenvalues and corresponding eigenvectors for the matrix $ \matrx{A} $.
		\label{def:eigenvalue_eigenvector} 
	\end{definition} 
	Before we can delve into a method for calculating the eigenvalues and eigenvectors, we need to explore another property of a matrix known as the ``determinant".
	% section on determinant
	\section{Determinant}
	The determinant is a property of a matrix that describes whether it is a non-singular (invertible) matrix. Consider the following set of simultaneous equations: 
	\begin{align}
		2x - 3y &= 4 \\
		5x + 7y &= 9
	\end{align}	
	% solving simultaneous equations using school method
	A standard way to solve this is by rearranging one of the equations, substituting this into the other equation and putting the derived value back into the original:	
	\begin{align*}
		x = \frac{1}{2} (4 + 3y) \\
		5\frac{1}{2}(4 + 3y) + 7y = 9 \\
		10 + \frac{15}{2} y + 7y = 9 \\
		10 + \frac{29}{2} y = 9 \\
		y = - \frac{29}{2} \tag{\nexteq} \\ 
		\\
		x = \frac{1}{2}(4 + 3(- \frac{29}{2})) \\
		x = \frac{55}{29} \tag{\nexteq}
	\end{align*}
	This method is sufficient when we are solving two equations with two unknowns but it becomes more tedious when we are working with larger systems of equations. An alternative method is to setup a matrix equation and use methods from linear algebra to determine the unknowns. We first setup our system by collecting together the coefficients rewriting the system: 
	% matrix representation of the set of equations
	\begin{equation}
		\begin{aligned}
			2x - 3y &= 4 \\
			5x + 7y &= 9
		\end{aligned}
		\xrightarrow{\text{Matrix Representation}}
		\begin{bmatrix}
			2 & -3 \\
			5 & 7 
		\end{bmatrix}
		\begin{bmatrix}
			x \\
			y
		\end{bmatrix}
		=
		\begin{bmatrix}
			4 \\
			9
		\end{bmatrix}
	\end{equation}
	
	This representation allows us to use all the tools that linear algebra provides. If we take our matrix equation and rearrange the variables, we get a way of deriving the solution for $ x $ and $ y $:
	% solution rearranging
	\begin{equation}
		\begin{array}{c}
			\begin{aligned}
				2x - 3y &= 4 \\
				5x + 7y &= 9
			\end{aligned}
			\\ \\
			\bigg\downarrow\vcenter{ }
			\\ \\
			\begin{aligned}
				\begin{bmatrix}
					2 & -3 \\
					5 & 7 
				\end{bmatrix}&
				\begin{bmatrix}
					x \\
					y
				\end{bmatrix}
				=
				\begin{bmatrix}
					4 \\
					9
				\end{bmatrix} \\
			\matrx{A} \vectr{v} &= \vectr{B} \\
			\vectr{v} &= \matrx{A}^{-1} \vectr{B}
			\end{aligned}
		\end{array}
	\end{equation}
	% inverse of a 2x2 matrix
	The inverse of a matrix $ \matrx{A} $, denoted as $ \matrx{A}^{-1} $, is a matrix that satisfies the equation $ \matrx{A} \matrx{A}^{-1} = \matrx{I} $ where $ \matrx{I} $ is the identity matrix. For the case of a matrix $ \matrx{A} \in \real^{2 \times 2} $, the inverse can be calculated using the ``inverse matrix formula": 
	\begin{equation}
		\matrx{A} = \begin{bmatrix}
			a & b \\
			c & d \\
		\end{bmatrix}
		\hspace{10pt}
		\matrx{A}^{-1} = \frac{1}{ad-bc} \begin{bmatrix}
			d & -b \\
			-c & a \\
		\end{bmatrix}
		\label{eq:2_2_inv}
	\end{equation}
	With this, we can calculate the solution to our system of linear equations:
	% matrix solution to the matrix equation
	\begin{align}
		\begin{bmatrix}
			x \\ 
			y
		\end{bmatrix} &= \begin{bmatrix}
			2 & -3 \\
			5 & 7 
		\end{bmatrix}^{-1} \begin{bmatrix}
			4 \\
			9
		\end{bmatrix} \\
		&= \frac{1}{(2)(7)-(-3)(5)} \begin{bmatrix}
			7 & 3 \\
			-5 & 2 \\
		\end{bmatrix} \begin{bmatrix}
			4 \\
			9
		\end{bmatrix} \\
		 &= \frac{1}{29} \begin{bmatrix}
			7 & 3 \\
			-5 & 2
		\end{bmatrix} \begin{bmatrix}
			4 \\
			9
		\end{bmatrix} \\
		&= \begin{bmatrix}
			55 \\
			-2
		\end{bmatrix} \\
		&= \begin{bmatrix}
			\frac{55}{29} \\
			- \frac{2}{29}
		\end{bmatrix} = \begin{bmatrix}
		x \\
		y
	\end{bmatrix}
	\end{align}
	This method could be considered a better choice because it generalises for much larger systems of linear equations. A part of the solution involved calculating the inverse of a matrix. In certain cases, the inverse of a matrix may not exist. This brings us to the determinant. The determinant is a scalar value derived as a function of all the elements of a matrix. It is only defined on squares matrices and if it is non-zero, the matrix is invertible. For a matrix $ \matrx{A} \in \real^{2 \times 2} $, the determinant of $ \matrx{A} $, denoted by $ \dett{A} $, is given by:
	% determinant of a 2x2 matrix
	\begin{align}
		\matrx{A} = \begin{bmatrix}
			a & b \\
			c & d
		\end{bmatrix}
		\hspace{10pt}
		\dett{A} = ad - bc
	\end{align}
	This makes sense for in the case of our system of linear equations. You can see in equation \ref{eq:2_2_inv} where the determinant features in the equation for the inverse of the matrix. If $ \dett{A} = 0$ then the inverse cannot be calculated. This same condition applies to equations for the inverse of larger matrices that don't explicitly involve $ \dett{A} $. For square matrices $ \matrx{A} \in \real^{n \times n}, \matrx{B} \in \real^{m \times m} $, the following properties hold:
	\begin{enumerate}
		\item $ \dett{AB} = \dett{A}\dett{B} $
		\item $ \dett{A}^{-1} = \frac{1}{\dett{A}} $
		\item $ \dett{A} = \dett{A^{\top}}$
	\end{enumerate}
	We can now revisit our discussion from section \ref{sec:eigen} on eigenvalues and eigenvectors. We can use the determinant to calculate these values. We can take the definition \ref{def:eigenvalue_eigenvector} and rewrite it into a form that we can solve:
	\begin{align}
		\matrx{A} \vectr{u} &= \lambda \vectr{u} \\
		\matrx{A} \vectr{u} &= \lambda \matrx{I} \vectr{u} \\
		(\matrx{A} - \lambda \matrx{I}) \vectr{u} &= \vectr{0} 
		\label{eq:proof_eigen_det}
	\end{align}
	\begin{proposition}
		\normalfont For a matrix $ \matrx{A} \in \real^{n \times n} $ and an eigenvalue $ \lambda \in \real $ with corresponding eigenvector $ \vectr{u} \in \real^{n} $: 
		\begin{align}
			\dett{\matrx{A} - \lambda \matrx{I}} = 0
		\end{align}
		\label{prop:det_eigen_eq}
	\end{proposition} 
	\italic{Proof}: Lets presume that the matrix $ \matrx{A} - \lambda \matrx{I} $ has an inverse so that $ (\matrx{A} - \lambda \matrx{I})^{-1} $ exists. Given \ref{eq:proof_eigen_det}, the following equation would be true: 
	\begin{align}
		\vectr{u} &= \matrx{I} \vectr{u} \\
		&= ((\matrx{A} - \lambda \matrx{I})^{-1} (\matrx{A} - \lambda \matrx{I})) \vectr{u} \\
		&= (\matrx{A} - \lambda \matrx{I})^{-1} ((\matrx{A} - \lambda \matrx{I}) \vectr{u}) \\
		&= (\matrx{A} - \lambda \matrx{I})^{-1} 0 \\
		&= 0
	\end{align}
	However, given that $ \vectr{u} \ne 0 $, we have a contradiction and the inverse of $ \matrx{A} - \lambda \matrx{I} $ cannot exist. Therefore $ \dett{\matrx{A} - \lambda \matrx{I}} = 0 $.
	We now have an equation involving the eigenvalues of a matrix that we can use to calculate that and the eigenvectors. Consider the following example for a matrix $ \matrx{A} \in \real^{2 \times 2} $:
	\begin{align}
		\matrx{A} = \begin{bmatrix}
			0 & 1 \\
		   -2 & -3 
		\end{bmatrix}
	\end{align}
	If we put this into $ \dett{\matrx{A} - \lambda \matrx{I}} = 0 $ then we can calculate our eigenvalues:
	\begin{align}
		\dett{\matrx{A} - \lambda \matrx{I}} &= \mdett{\begin{bmatrix}
				0 & 1 \\
				-2 & -3 
			\end{bmatrix} - \lambda \begin{bmatrix}
				1 & 0 \\
				0 & 1
		\end{bmatrix}} \\ &= \mdett{\begin{bmatrix}
				-\lambda & 1 \\
				-2 & -3 - \lambda
		\end{bmatrix}} \\
		&= (-\lambda)(-3-\lambda) - (1)(-2) \\
		\label{eq:char_det} 0 &= \lambda^{2} + 3\lambda + 2 \\
		0 &= (\lambda + 2)(\lambda + 1) \Rightarrow \lambda_1 = -2, \lambda_2 = -1
	\end{align}
	Now that we have our eigenvalues, we can use equation \ref{eq:proof_eigen_det} to find the corresponding eigenvectors:
	\begin{align}
		\text{for } \lambda_1 = -1:&
		(\matrx{A} - \lambda \matrx{I}) \vectr{u}_1 = \vectr{0} \\
		& \begin{bmatrix}
			1 & 1 \\
			-2 & -2
		\end{bmatrix} \begin{bmatrix}
			\vectr{u}_{1,1} \\
			\vectr{u}_{1,2}
		\end{bmatrix} = \begin{bmatrix}
			0 \\
			0
		\end{bmatrix} \\
		&\therefore \vectr{u}_{1,1} = -\vectr{u}_{1,2} \Rightarrow \vectr{u}_1 = \begin{bmatrix}
			1 \\
			-1
		\end{bmatrix} \\ \\
		\text{for } \lambda_1 = -2:&
		(\matrx{A} - \lambda \matrx{I}) \vectr{u}_2 = \vectr{0} \\
		& \begin{bmatrix}
			2 & 1 \\
			-2 & -1
		\end{bmatrix} \begin{bmatrix}
			\vectr{u}_{2,1} \\
			\vectr{u}_{2,2}
		\end{bmatrix} = \begin{bmatrix}
			0 \\
			0
		\end{bmatrix} \\
		&\therefore \vectr{u}_{2,1} = -2 \vectr{u}_{2,2} \Rightarrow \vectr{u}_2 = \begin{bmatrix}
			1 \\
			-2
		\end{bmatrix}
	\end{align}
	\begin{proposition}
		\normalfont Given a matrix $ \matrx{A} $ with distinct eigenvalues $ \{\lambda_1,\lambda_2,\ldots \\,\lambda_{k - 1},\lambda_k\} $ and corresponding eigenvectors $ \{\vectr{v}_1,\vectr{v}_2,\ldots,\vectr{v}_{k - 1},\vectr{v}_k\} $, the set of eigenvectors are linearly independent.
		\label{prop:eigenvectors_linearly_independent}
	\end{proposition}
	\italic{Proof}: From definition \ref{def:eigenvalue_eigenvector} we know that each eigenvalue and eigenvector pair satisfies $ \matrx{A}\vectr{v}_i = \lambda_i \vectr{v}_i \forall i = 1,\ldots,k$. from definition \ref{def:linear_independence} we know that a set of vectors is linearly independent if no non-trivial linear combinations (see definition \ref{def:linear_combinations}) equate to the zero vector. Knowing these definitions, we can use induction to prove the proposition.
	Consider the case when $ k = 1 $. A set with only one vector $ \vectr{v}_1 $ is always linearly independent. Now presume that in the case of $ k = i $, the set of vectors $ \{\vectr{v}_1,\vectr{v}_2,\ldots,\vectr{v}_{i - 1},\vectr{v}_i\} $ is linearly independent. if we now set $ k = i + 1 $, we are concerned with determining if there is any non-trivial linear combinations that result in the zero vector in equation \ref{eq:eigenvector_linear_independence_k_1}. 
	\begin{align}
		a_1 \vectr{v}_1 + a_2 \vectr{v}_2 + \ldots + a_{i - 1} \vectr{v}_{i - 1} + a_i \vectr{v}_i + a_{i + 1} \vectr{v}_{i + 1} = \vectr{0}
		\label{eq:eigenvector_linear_independence_k_1}
	\end{align}
	Firstly, we can alter this equation in two ways:. Firstly, we multiply both sides by $ \matrx{A} $ and simplify:
	\begin{align}
		\matrx{A}(a_1 \vectr{v}_1 + a_2 \vectr{v}_2 + \ldots + a_{i - 1} \vectr{v}_{i - 1} + a_i \vectr{v}_i + a_{i + 1} \vectr{v}_{i + 1}) &= \matrx{A}\vectr{0} \nonumber \\
		a_1 \matrx{A}\vectr{v}_1 + a_2 \matrx{A}\vectr{v}_2 + \ldots + a_{i - 1} \matrx{A}\vectr{v}_{i - 1} + a_i \matrx{A}\vectr{v}_i + a_{i + 1} \matrx{A}\vectr{v}_{i + 1} &= \vectr{0} \nonumber \\
		\label{eq:eigenvector_linear_independence_eq_1} a_1 \lambda_1 + a_2 \lambda_2 + \ldots + a_{i - 1} \lambda_{i - 1} + a_i \lambda_i + a_{i + 1} \lambda_{i + 1} &= \vectr{0} 
	\end{align}
	Secondly, we can multiply both sides by $ \lambda_{i + 1} $:
	\begin{align}
		\lambda_{i + 1}(a_1 \vectr{v}_1 + a_2 \vectr{v}_2 + \ldots + a_{i - 1} \vectr{v}_{i - 1} + a_i \vectr{v}_i + a_{i + 1} \vectr{v}_{i + 1}) &= \lambda_{i + 1}\vectr{0} \nonumber \\
		\label{eq:eigenvector_linear_independence_eq_2} a_1\lambda_{i + 1} \vectr{v}_1 + a_2\lambda_{i + 1} \vectr{v}_2 + \ldots +  + a_i\lambda_{i + 1} \vectr{v}_i + a_{i + 1}\lambda_{i + 1} \vectr{v}_{i + 1} &= \vectr{0}
	\end{align}
	If we subtract equation \ref{eq:eigenvector_linear_independence_eq_2} from \ref{eq:eigenvector_linear_independence_eq_1}:
	\begin{align}
		a_1 (\lambda_1 - \lambda_{i + 1}) \vectr{v}_1 + a_2 (\lambda_2 - \lambda_{i + 1}) \vectr{v}_2 + \ldots \nonumber \\+ a_i (\lambda_i - \lambda_{i + 1}) \vectr{v}_i + a_{i + 1} (\lambda_{i + 1} - \lambda_{i + 1}) \vectr{v}_{i + 1} &= \vectr{0} 
	\end{align}
	Since the difference between any two distinct eigenvalues is non-zero and all eigenvectors are non-zero, $ a_1 = a_2 = \ldots = a_i = 0 $. substituting these into equation \ref{eq:eigenvector_linear_independence_k_1}, This is reduced down to $ a_{i + 1}\vectr{v}_{i + 1} = \vectr{0}$. Since $ \vectr{v}_{i + 1} \ne 0 $,  $ a_{i + 1} = 0 $. This means that in the case of $ k = i + 1 $, the eigenvectors are linearly independent and proposition \ref{prop:eigenvectors_linearly_independent} is proven by induction.
	
	In equation \ref{eq:char_det}, we have a polynomial, the solutions of which are the eigenvalues of the matrix. This is called the characteristic equation. It is always the case that a square matrix of size $ n $ will have an $ n $th degree polynomial as its characteristic equation. This leads us to an important finding.
	
	\begin{proposition}
		\normalfont Given a matrix $ \matrx{A} \in \real^{n \times n} $ where $ n \in \natral $ and eigenvalues $ \lambda_1,\ldots,\lambda_k $: 
		\begin{align}
			\dett{\matrx{A}} = \prod_{i = 1}^{k} \lambda_i
		\end{align}
		where $ k \leq n $. 
		\label{prop:det_prod_eigen}
	\end{proposition}
	\italic{Proof}: 
	\begin{align}
		&\dett{\matrx{A} - \lambda \matrx{I}} = P(\lambda) \\ 
		&= (\lambda_1 - \lambda)(\lambda_2 - \lambda)\ldots(\lambda_{n - 1} - \lambda)(\lambda_n - \lambda)
	\end{align}
	If we let $ \lambda = 0$:
	\begin{align}
		\dett{A - \lambda\matrx{I}} &= P(\lambda) \\
		\dett{A} &= P(0) \\
		&= (\lambda_1 - 0)(\lambda_2 - 0)\ldots(\lambda_{n - 1} - 0)(\lambda_n - 0) \\
		&= \prod_{i = 1}^{n} \lambda_i
	\end{align}
	And therefore, proposition \ref{prop:det_prod_eigen} is true. Furthermore, this fact can allow us to demonstrate the connection between the determinant of a matrix and whether it is invertible.
	\begin{proposition}
		\normalfont Given a matrix $ \matrx{A} \in \real^{n \times n} $ where $ n \in \natral $, $ \matrx{A}^{-1} $ exists $\iff$ $ \dett{A} \ne 0 $.
		\label{prop:det_eq_inv}
	\end{proposition}
	\italic{Proof}: Lets presume that $ \matrx{A}^{-1} $ exists. Then $ \matrx{A}\vectr{w} = \vectr{f} $ has a unique solution $ \vectr{w} = \matrx{A}^{-1}\vectr{f} $ for every $ \vectr{f} $.  We know that from proposition \ref{prop:det_prod_eigen} the only way $ \dett{A} = 0 $ is if one of its eigenvalues is $ 0 $. From definition \ref{prop:det_eigen_eq}, we know that $ (\matrx{A} - \lambda\matrx{I})\vectr{v} = 0 $ for any eigenvalue $ \lambda $ of $ \matrx{A} $ and its corresponding eigenvector $ \vectr{v} $. If one of our eigenvalues is equal to $ 0 $, then $ (\matrx{A} - 0\matrx{I})\vectr{v} = 0$ and $ \matrx{A}\vectr{v} = 0 $. If $ \matrx{A}\vectr{w} = \vectr{f} $ then we can say that $ \matrx{A}(\vectr{w} + \vectr{v}) = \vectr{f} $. This means that if $ \matrx{A} $ has an inverse and $ \dett{A} = 0 $, $ \matrx{A}\vectr{w} = \vectr{f} $ has multiple solutions, namely $ \vectr{w} $ and $ \vectr{w} + \vectr{v} $. Therefore, either the inverse does not exist or $ \dett{A} \ne 0 $. In other words, the inverse of $ \matrx{A} $ only exists when $ \dett{A} \ne 0$ and proposition \ref{prop:det_eq_inv} is proven. 

	We can summarise how to calculate determinants for larger matrices. Given a square matrix $ \matrx{A} \in \real^{n \times n} $:
	\begin{enumerate}
		\item $ n = 1 $: $ \dett{A} = \dett{a_{11}} $.
		\item $ n = 2 $: $ \dett{A} = a_{11}a_{22} - a_{12}a_{21} $.
		\item $ n = 3 $: $ \dett{A} = a_{11}a_{22}a_{33} + a_{21}a_{32}a_{13} + a_{31}a_{12}a_{23} - a_{31}a_{22}a_{13} - a_{11}a_{32}a_{23} - a_{21}a_{12}a_{33}$.
		\item For $ n > 3 $: \begin{theorem}
			\normalfont (Laplace Expansion). Consider a matrix $ \matrx{A} \in \real^{n \times n} $. Then, for all $ j = 1,\ldots,n $:
			\begin{enumerate}
				\item Expansion along column $ j $:
				\begin{align}
					\dett{A} = \sum_{k = 1}^{n} (-1)^{k + j} a_{kj} \dett{A_{k,j}}
				\end{align}
				\item Expansion along row $ j $:
				\begin{align}
					\dett{A} = \sum_{k = 1}^{n} (-1)^{k + j} a_{jk} \dett{A_{j,k}}
				\end{align}
			\end{enumerate}
		Where $ \matrx{A}_{k,j} \in \real^{(n - 1) \times (n - 1)} $ is the submatrix of $ \matrx{A} $ that we obtain when deleting row $ k $ and column $ j $. 
		\end{theorem}
	\end{enumerate}
	
	\section{Linear Transformations}
	Linear transformations are concerned with going from one vector space to another. During the transformation, the properties of a vector space (defined in \ref{def:vector_space}) are maintained. 
	
	\begin{definition}
		\normalfont A linear transformation between two vector spaces $ \vecspace{V} $ and $ \vecspace{W} $ is a map $ \Psi: \vecspace{V} \rightarrow \vecspace{W} $ such that the following hold:
		\begin{enumerate}
			\item $ \Psi(\vectr{v}_1 + \vectr{v}_2) = \Psi(\vectr{v}_1) + \Psi(a\vectr{v}_2)$ for any vectors $ \vectr{v}_1 $ and $ \vectr{v}_2 $ in $ \vecspace{V} $.
			\item $ \Psi(\alpha \vectr{v}) = \alpha \Psi(\vectr{v})$ for any scalar $ \alpha \in \real $.  
		\end{enumerate}
		After transformation is applied, straight lines in the first vector space will remain as straight lines in the final vector space. It is also the case that $ \Psi(0) = 0 $. There are several different types of linear transformations (or linear mappings) but we will restrict our focus to a small few. Given two vector spaces $ \vecspace{V} $ and $ \vecspace{W} $ and a linear transformation $ \Phi: \vecspace{V} \rightarrow \vecspace{W} $:
		\begin{enumerate}
			\item (Injective) If $ \forall \vectr{x}, \vectr{y} \in \vecspace{V}: \Phi(\vectr{x}) = \Phi(\vectr{y}) \Longrightarrow \vectr{x} = \vectr{y}$. What this means is that distinct elements in $ \vecspace{V} $ are mapped onto distinct elements in $ \vecspace{W} $. It is not necessarily the case that every element in $ \vecspace{W} $ can be reached from $ \vecspace{V} $. 
			\item (Surjective) If $ \Phi(\vecspace{V}) = \vecspace{W}$. What this means is that every element in $ \vecspace{W} $ can be reached from $ \vecspace{V} $. This doesn't need to be a unique relationship. 
			\item (Bijective) If $ \Phi $ is bijective, then it is injective and surjective. ``Bi" represents that elements come in pairs; one from $ \vecspace{V} $ and one from $ \vecspace{W} $ such that there is a unique relationship between elements from one vector space to the other. It is also the case that every element in $ \vecspace{W} $ can be reached from $ \vecspace{V} $.
		\end{enumerate}
	\end{definition}
	Given these definitions, we can describe the type of transformation that we are going to focus on: isomorphic transformations. This means that the transformation is linear (i.e. straight lines are maintained and zero is unchanged) and bijective. 
	A transformation allows us to go from one coordinate system to another. It is important to clarify what a coordinate actually is. 
	\begin{definition}[Coordinate]
		\normalfont Consider a vector space $ V $ and an ordered basis $ B = (\vectr{b}_1, \ldots, \vectr{b}_n) $ of $ V $. For any $ \vectr{x} \in V $ we obtain a unique representation (linear combination):
		\begin{align}
			\vectr{x} = \alpha \vectr{b}_1 + \ldots + \alpha_n \vectr{b}_n 
		\end{align}
		of $ \vectr{x} $ with respect to $ B $. Then $ \alpha_1, \ldots, \alpha_n $ are the coordinates of $ \vectr{x} $ with respect to $ B $, and the vector
		\begin{align}
			\boldsymbol{\alpha} = \begin{bmatrix}
				\alpha_1 \\
				\vdots \\
				\alpha_n
			\end{bmatrix} \in \real^{n} 
		\end{align}
	is the coordinate vector / coordinate representation of $ \vectr{x} $ with respect to the ordered basis $ B $.
	\label{def:coordinate}
	\end{definition}
	In the case of $ \real^{2} $, we have discussed previously how every vector $ \vectr{v} $ can be represented as linear combinations of $ \vectr{e}_1 = (1, 0)^{\top} $ and $ \vectr{e}_2 = (0, 1)^{\top} $ such that $ \forall \vectr{v} \in \real^{2} $, there exists $ \alpha_1, \alpha_2 \in \real $ such that $ \vectr{v} = \alpha_1 \vectr{e}_1 + \alpha_2 \vectr{e}_2 $. In this case (and in the case of all ordered sets of basis vectors) the coordinate would be $ (\alpha_1, \alpha_2) $. This is why you often see coordinates in applied mathematics represented as $ \alpha_1 \vectr{i} + \alpha_2 \vectr{j} $. Lets suppose we had a vector space $ V $ and two sets of vectors $ \set{A} = \{\vectr{a}_1,\ldots,\vectr{a}_n\} $ and $ \set{B} = \{\vectr{b}_1,\ldots,\vectr{b}_n\} $.  By definition \ref{def:basis}, we can write each vector in $ \set{A} $ as a unique linear combination of the vectors in $ \set{B} $. Given scalars $ c_{ij} $ where $ 1 \leq i,j \leq n $:
	\begin{align*}
		\vectr{a}_1 =& c_{1,1}\vectr{b}_1 + c_{1,2}\vectr{b}_{1,2} + \ldots + c_{1,n}\vectr{u}_n \\
		  &\vdots \hspace{50pt} \vdots \hspace{50pt} \vdots \\ 
		 \vectr{a}_n =& c_{n,1}\vectr{b}_1 + c_{n,2}\vectr{b}_{1,2} + \ldots + c_{n,n}\vectr{u}_n 
	\end{align*}
	Lets introduce a matrix $ \matrx{G} = (c_{ij})_{ij}$ which we will call the \textbf{change-of-basis matrix}. Any vector $ \vectr{v} $ can now be represented in the following way:
	\begin{align}
		\vectr{a} &= (\vectr{a}_1 \hspace{3mm} \vectr{a}_2 \hspace{3mm} \ldots \hspace{3mm} \vectr{a}_n) \begin{bmatrix}
			c_1 \\ c_2 \\ \vdots \\ c_n
		\end{bmatrix} \\
		&= (\vectr{b}_1 \hspace{3mm} \vectr{b}_2 \hspace{3mm} \ldots \hspace{3mm} \vectr{b}_n) \hspace{1mm} \matrx{G} \hspace{1mm} \begin{bmatrix}
			c_1 \\ c_2 \\ \vdots \\ c_n
		\end{bmatrix}
	\end{align} 
	In other words, to construct a matrix which allows us to transform any coordinate constructed from the basis vectors in $ \set{B} $ to those in $ \set{A} $, we can use the coordinates of the basis vectors themselves to define the transformation. Furthermore, This transformation is injective.
	\\\\
	\italic{Example}: Consider a vector space $ V $ and a vector $ \vectr{v} = 15\vectr{e}_1 - 5\vectr{e}_2 $ where $ \vectr{e}_1 = (1, 0)^{\top} $ and $ \vectr{e}_2 = (0, 1)^{\top} $ form a basis for $ V $. We want to take the coordinate $(15, -5)$ and represent it using two new basis vectors of $ V $, $ \vectr{d}_1 = (1, 1)^{\top} $ and $ \vectr{d}_2 = (3, -2)^{\top} $. From definition \ref{def:basis} and definition \ref{def:coordinate}, we want to find $ \alpha, \beta \in \real $ such that: 
	\begin{align}
		\begin{bmatrix}
			15 \\ 2
		\end{bmatrix} = \alpha \begin{bmatrix}
			1 \\ 1 
		\end{bmatrix} + \beta \begin{bmatrix}
		    3 \\ -2
		\end{bmatrix} &= \begin{bmatrix}
			1 & 3 \\ 
			1 & -2 
		\end{bmatrix} \begin{bmatrix}
			\alpha \\ \beta
		\end{bmatrix} \\
		\begin{bmatrix}
			1 & 3 \\
			1 & -2 
		\end{bmatrix}^{-1} 
		\begin{bmatrix}
			15 \\ -5
		\end{bmatrix}
		&= \begin{bmatrix}
			3 \\ 4
		\end{bmatrix}
	\end{align}
	Therefore, the coordinate $ (15, -5) $ with respect to the basis vectors $ \vectr{e}_1 $ and $ \vectr{e}_2 $ is equivalent to $ (3, 4) $ with respect to the basis vectors $ \vectr{d}_1 $ and $ \vectr{d}_2 $. \\	
	In this example, an important concept appears. Linear transformations can be represented as a matrix. More precisely, this is how they're defined. Suppose we wanted to define a linear transformation as a matrix that represented a reflection in the line $ y = x $. Let $ V $ be a vector space and $ \vectr{v} = (\alpha, \beta) $ be a vector in that vector space. $ V $ has basis vectors $ \vectr{d}_1 $ and $ \vectr{d}_2 $ meaning that $ \vectr{v} = \alpha \vectr{d}_1 + \beta \vectr{d}_2 $. Let $ W $ be a vector space and $ \vectr{w} = (\delta, \epsilon`) $ be a vector in that vector space. $ W $ has basis vectors $ \vectr{f}_1 $ and $ \vectr{f}_2 $ meaning that $ \vectr{v} = \delta \vectr{f}_1 + \epsilon \vectr{f}_2 $. Let $ \matrx{A} $ be the matrix that defines the reflection in the line $ y = x $:
	\begin{align}
		\vectr{v} &= \alpha \vectr{d}_1 + \beta \vectr{d}_2 \\
		\matrx{M} \vectr{v} &= \matrx{M} [ \alpha \vectr{d}_1 + \beta \vectr{d}_2] \\
		 &= \alpha \matrx{M} \vectr{d}_1 + \beta \matrx{M} \vectr{d}_2 \\
		 &= \alpha \vectr{f}_1 + \beta \vectr{f}_2
	\end{align}
	What this demonstrates is that if we consider how the basis vectors $ \vectr{d}_1 $ and $ \vectr{d}_2 $ for $ V $ are changed after the described transformation, we can construct the matrix $ \matrx{M} $ with each column vector being the ordered basis vectors $ \vectr{f}_1 $ and $ \vectr{f}_2 $. This then allows us to perform the reflection on any coordinate in $ V $. 
	\section{Matrix Decompositions}
	
  	\printbibliography
\end{document}